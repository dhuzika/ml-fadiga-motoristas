{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treino XGBoost - Detecção de Fadiga\n",
    "\n",
    "Treina modelo XGBoost para classificação binária: Alerta vs Sonolento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotecas carregadas\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy import stats\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Bibliotecas carregadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados carregados: 32111 amostras\n",
      "Shape das sequences: (32111, 90, 4)\n",
      "Labels únicas: [ 0  5 10]\n"
     ]
    }
   ],
   "source": [
    "# Carrega dados processados\n",
    "dados_dir = Path(\"processed_data_uta_rldd_CORRECTED\")\n",
    "arquivo_X = dados_dir / \"X_sequences.npy\"\n",
    "arquivo_y = dados_dir / \"y_labels.npy\"\n",
    "\n",
    "if not arquivo_X.exists() or not arquivo_y.exists():\n",
    "    print(\"Erro: Execute primeiro o notebook de preparação dos dados\")\n",
    "    print(f\"Arquivos necessários:\")\n",
    "    print(f\"  {arquivo_X}\")\n",
    "    print(f\"  {arquivo_y}\")\n",
    "    raise FileNotFoundError(\"Arquivos de dados não encontrados\")\n",
    "\n",
    "# Carrega sequences e labels\n",
    "X_sequences = np.load(arquivo_X)\n",
    "y_labels = np.load(arquivo_y)\n",
    "\n",
    "print(f\"Dados carregados: {X_sequences.shape[0]} amostras\")\n",
    "print(f\"Shape das sequences: {X_sequences.shape}\")\n",
    "print(f\"Labels únicas: {np.unique(y_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribuição original:\n",
      "  0: 10826 amostras\n",
      "  5: 10473 amostras\n",
      "  10: 10812 amostras\n",
      "\n",
      "Distribuição binária:\n",
      "  Alerta (0): 21299 amostras\n",
      "  Sonolento (1): 10812 amostras\n"
     ]
    }
   ],
   "source": [
    "# Converte para classificação binária\n",
    "# 0,5 -> 0 (Alerta), 10 -> 1 (Sonolento)\n",
    "y_binario = np.where(y_labels == 10, 1, 0)\n",
    "\n",
    "print(\"Distribuição original:\")\n",
    "unique, counts = np.unique(y_labels, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  {label}: {count} amostras\")\n",
    "\n",
    "print(\"\\nDistribuição binária:\")\n",
    "unique, counts = np.unique(y_binario, return_counts=True)\n",
    "labels_texto = ['Alerta', 'Sonolento']\n",
    "for label, count, texto in zip(unique, counts, labels_texto):\n",
    "    print(f\"  {texto} ({label}): {count} amostras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extrator criado - 44 features por amostra\n"
     ]
    }
   ],
   "source": [
    "# Extrai features estatísticas das sequences\n",
    "class ExtratorFeatures:\n",
    "    def __init__(self):\n",
    "        nomes_sinais = ['PERCLOS', 'MAR', 'BLINK_RATE', 'HEAD_STABILITY']\n",
    "        nomes_stats = ['mean', 'std', 'median', 'min', 'max', 'range', 'q25', 'q75', 'trend', 'zcr', 'autocorr']\n",
    "        \n",
    "        self.feature_names = []\n",
    "        for sinal in nomes_sinais:\n",
    "            for stat in nomes_stats:\n",
    "                self.feature_names.append(f\"{sinal}_{stat}\")\n",
    "    \n",
    "    def trend_slope(self, sinal):\n",
    "        if len(sinal) < 2:\n",
    "            return 0.0\n",
    "        x = np.arange(len(sinal))\n",
    "        try:\n",
    "            slope, _, _, _, _ = stats.linregress(x, sinal)\n",
    "            return slope if not np.isnan(slope) else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def zero_crossing_rate(self, sinal):\n",
    "        if len(sinal) < 2:\n",
    "            return 0.0\n",
    "        mean_centered = sinal - np.mean(sinal)\n",
    "        crossings = np.sum(np.diff(np.sign(mean_centered)) != 0)\n",
    "        return crossings / len(sinal)\n",
    "    \n",
    "    def autocorr_lag1(self, sinal):\n",
    "        if len(sinal) < 3:\n",
    "            return 0.0\n",
    "        try:\n",
    "            corr = np.corrcoef(sinal[:-1], sinal[1:])[0, 1]\n",
    "            return corr if not np.isnan(corr) else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def extrair_features_sinal(self, sinal):\n",
    "        features = [\n",
    "            np.mean(sinal), np.std(sinal), np.median(sinal),\n",
    "            np.min(sinal), np.max(sinal), np.ptp(sinal),\n",
    "            np.percentile(sinal, 25), np.percentile(sinal, 75),\n",
    "            self.trend_slope(sinal), self.zero_crossing_rate(sinal), self.autocorr_lag1(sinal)\n",
    "        ]\n",
    "        return features\n",
    "    \n",
    "    def transform(self, X_sequences):\n",
    "        n_samples = X_sequences.shape[0]\n",
    "        n_features = len(self.feature_names)\n",
    "        X_features = np.zeros((n_samples, n_features))\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            sequence = X_sequences[i]\n",
    "            sample_features = []\n",
    "            \n",
    "            for signal_idx in range(4):\n",
    "                sinal = sequence[:, signal_idx]\n",
    "                features_sinal = self.extrair_features_sinal(sinal)\n",
    "                sample_features.extend(features_sinal)\n",
    "            \n",
    "            X_features[i] = sample_features\n",
    "        \n",
    "        return X_features\n",
    "\n",
    "extrator = ExtratorFeatures()\n",
    "print(f\"Extrator criado - {len(extrator.feature_names)} features por amostra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraindo features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhizhu/tcc/ia-fadiga-main2 (copy)/fatigue_env/lib/python3.11/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/zhizhu/tcc/ia-fadiga-main2 (copy)/fatigue_env/lib/python3.11/site-packages/numpy/lib/function_base.py:2855: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features extraídas: (32111, 44)\n",
      "Treino: 25688 amostras\n",
      "Teste: 6423 amostras\n"
     ]
    }
   ],
   "source": [
    "# Extrai features e divide dados\n",
    "print(\"Extraindo features...\")\n",
    "X_features = extrator.transform(X_sequences)\n",
    "print(f\"Features extraídas: {X_features.shape}\")\n",
    "\n",
    "# Divide treino/teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y_binario, test_size=0.2, random_state=42, stratify=y_binario\n",
    ")\n",
    "\n",
    "print(f\"Treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Teste: {X_test.shape[0]} amostras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features normalizadas\n"
     ]
    }
   ],
   "source": [
    "# Normaliza features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features normalizadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treinando XGBoost...\n",
      "Peso Alerta: 0.75\n",
      "Peso Sonolento: 1.49\n",
      "Modelo treinado!\n"
     ]
    }
   ],
   "source": [
    "# Treina XGBoost\n",
    "print(\"Treinando XGBoost...\")\n",
    "\n",
    "# Calcula pesos das classes\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "total = len(y_train)\n",
    "peso_alerta = total / (2 * counts[0]) if 0 in unique else 1.0\n",
    "peso_sonolento = total / (2 * counts[1]) if 1 in unique else 1.0\n",
    "\n",
    "print(f\"Peso Alerta: {peso_alerta:.2f}\")\n",
    "print(f\"Peso Sonolento: {peso_sonolento:.2f}\")\n",
    "\n",
    "modelo_xgb = xgb.XGBClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=6,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_alpha=0.1,\n",
    "    reg_lambda=0.1,\n",
    "    scale_pos_weight=peso_sonolento/peso_alerta,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "\n",
    "modelo_xgb.fit(X_train_scaled, y_train)\n",
    "print(\"Modelo treinado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultados:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Alerta       0.92      0.88      0.90      4260\n",
      "   Sonolento       0.79      0.85      0.82      2163\n",
      "\n",
      "    accuracy                           0.87      6423\n",
      "   macro avg       0.85      0.87      0.86      6423\n",
      "weighted avg       0.87      0.87      0.87      6423\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "         Alerta  Sonolento\n",
      "Alerta     3758       502\n",
      "Sonolento   327      1836\n",
      "\n",
      "Acurácia: 0.871\n"
     ]
    }
   ],
   "source": [
    "# Avalia modelo\n",
    "y_pred = modelo_xgb.predict(X_test_scaled)\n",
    "y_proba = modelo_xgb.predict_proba(X_test_scaled)\n",
    "\n",
    "print(\"\\nResultados:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Alerta', 'Sonolento']))\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(f\"         Alerta  Sonolento\")\n",
    "print(f\"Alerta     {cm[0,0]:4d}      {cm[0,1]:4d}\")\n",
    "print(f\"Sonolento  {cm[1,0]:4d}      {cm[1,1]:4d}\")\n",
    "\n",
    "accuracy = (cm[0,0] + cm[1,1]) / cm.sum()\n",
    "print(f\"\\nAcurácia: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Salva modelo e componentes\nmodelo_dir = Path(\"modelos_xgb\")\nmodelo_dir.mkdir(exist_ok=True)\n\n# Salva modelo\njoblib.dump(modelo_xgb, modelo_dir / \"modelo_xgb.joblib\")\n\n# Salva scaler\njoblib.dump(scaler, modelo_dir / \"scaler.joblib\")\n\n# Não salva extrator - será criado no pipeline\n\n# Info das classes\nclass_info = {\n    \"classes\": [0, 1],\n    \"class_names\": [\"Alerta\", \"Sonolento\"],\n    \"accuracy\": float(accuracy)\n}\n\nwith open(modelo_dir / \"info_classes.json\", 'w') as f:\n    json.dump(class_info, f, indent=2)\n\nprint(f\"Modelo salvo em: {modelo_dir}\")\nprint(\"Arquivos criados:\")\nprint(\"  - modelo_xgb.joblib\")\nprint(\"  - scaler.joblib\")\nprint(\"  - info_classes.json\")"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline de predição criado: pipeline.py\n",
      "\n",
      "Treino concluído! Execute o notebook de teste em tempo real.\n"
     ]
    }
   ],
   "source": [
    "# Cria pipeline de predição\n",
    "pipeline_code = '''import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class PipelineFadiga:\n",
    "    def __init__(self, modelo_dir):\n",
    "        modelo_dir = Path(modelo_dir)\n",
    "        \n",
    "        self.extrator = joblib.load(modelo_dir / \"extrator.joblib\")\n",
    "        self.scaler = joblib.load(modelo_dir / \"scaler.joblib\")\n",
    "        self.modelo = joblib.load(modelo_dir / \"modelo_xgb.joblib\")\n",
    "        \n",
    "        with open(modelo_dir / \"info_classes.json\", \"r\") as f:\n",
    "            self.class_info = json.load(f)\n",
    "    \n",
    "    def predict_sequence(self, sequence):\n",
    "        \"\"\"Prediz fadiga de uma sequence (90, 4)\"\"\"\n",
    "        sequences = np.expand_dims(sequence, axis=0)\n",
    "        features = self.extrator.transform(sequences)\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        prediction = self.modelo.predict(features_scaled)[0]\n",
    "        probabilities = self.modelo.predict_proba(features_scaled)[0]\n",
    "        \n",
    "        prob_dict = {\n",
    "            \"Alerta\": float(probabilities[0]),\n",
    "            \"Sonolento\": float(probabilities[1])\n",
    "        }\n",
    "        \n",
    "        class_name = \"Alerta\" if prediction == 0 else \"Sonolento\"\n",
    "        \n",
    "        return prediction, prob_dict, class_name\n",
    "'''\n",
    "\n",
    "with open(modelo_dir / \"pipeline.py\", 'w') as f:\n",
    "    f.write(pipeline_code)\n",
    "\n",
    "print(\"Pipeline de predição criado: pipeline.py\")\n",
    "print(\"\\nTreino concluído! Execute o notebook de teste em tempo real.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}