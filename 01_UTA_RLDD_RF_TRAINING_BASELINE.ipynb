{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento Random Forest - Baseline UTA-RLDD\n",
    "\n",
    "Este notebook implementa um modelo baseline usando Random Forest para detec√ß√£o de fadiga no dataset UTA-RLDD.\n",
    "\n",
    "**Pr√©-requisito**: Execute o notebook de prepara√ß√£o de dados primeiro.\n",
    "\n",
    "**Meta**: Atingir 75-85% de acur√°cia como baseline\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importa√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas principais\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "\n",
    "# An√°lise estat√≠stica\n",
    "from scipy import stats\n",
    "\n",
    "print(\"Bibliotecas carregadas com sucesso!\")\n",
    "print(f\"Diret√≥rio atual: {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configura√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caminhos dos dados\n",
    "DIRETORIO_DADOS = Path(\"processed_data_uta_rldd_CORRECTED\")\n",
    "DIRETORIO_MODELOS = DIRETORIO_DADOS / \"rf_models\"\n",
    "DIRETORIO_MODELOS.mkdir(exist_ok=True)\n",
    "\n",
    "# Nomes das caracter√≠sticas originais\n",
    "NOMES_SINAIS = ['PERCLOS', 'MAR', 'BLINK_RATE', 'HEAD_STABILITY']\n",
    "NOMES_STATS = ['mean', 'std', 'median', 'min', 'max', 'range', 'q25', 'q75', 'trend', 'zcr', 'autocorr']\n",
    "\n",
    "# Criar nomes das features\n",
    "NOMES_FEATURES = []\n",
    "for sinal in NOMES_SINAIS:\n",
    "    for stat in NOMES_STATS:\n",
    "        NOMES_FEATURES.append(f\"{sinal}_{stat}\")\n",
    "\n",
    "print(f\"Total de features a extrair: {len(NOMES_FEATURES)}\")\n",
    "print(f\"Diret√≥rio dos dados: {DIRETORIO_DADOS}\")\n",
    "print(f\"Diret√≥rio dos modelos: {DIRETORIO_MODELOS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carregamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carregar_dados_processados():\n",
    "    \"\"\"Carrega sequ√™ncias e labels do notebook de prepara√ß√£o de dados\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Tentar carregar arquivos numpy\n",
    "        arquivo_X = DIRETORIO_DADOS / \"X_sequences.npy\"\n",
    "        arquivo_y = DIRETORIO_DADOS / \"y_labels.npy\"\n",
    "        \n",
    "        if arquivo_X.exists() and arquivo_y.exists():\n",
    "            X_sequences = np.load(arquivo_X)\n",
    "            y_labels = np.load(arquivo_y)\n",
    "            print(f\"Dados carregados de {arquivo_X} e {arquivo_y}\")\n",
    "            return {'X_sequences': X_sequences, 'y_labels': y_labels}\n",
    "        \n",
    "        # Listar arquivos dispon√≠veis\n",
    "        arquivos_disponiveis = list(DIRETORIO_DADOS.glob(\"*.pkl\")) + list(DIRETORIO_DADOS.glob(\"*.npy\"))\n",
    "        print(f\"Arquivos dispon√≠veis em {DIRETORIO_DADOS}:\")\n",
    "        for arquivo in arquivos_disponiveis:\n",
    "            print(f\"  - {arquivo.name}\")\n",
    "        \n",
    "        raise FileNotFoundError(\"N√£o foi poss√≠vel encontrar os dados processados\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao carregar dados: {e}\")\n",
    "        print(\"Execute o notebook de prepara√ß√£o de dados primeiro.\")\n",
    "        raise\n",
    "\n",
    "# Carregar os dados\n",
    "dados = carregar_dados_processados()\n",
    "\n",
    "X_sequences = dados['X_sequences']\n",
    "y_labels = dados['y_labels']\n",
    "\n",
    "print(f\"Formato das sequ√™ncias: {X_sequences.shape}\")\n",
    "print(f\"Formato dos labels: {y_labels.shape}\")\n",
    "print(f\"Labels √∫nicos: {np.unique(y_labels)}\")\n",
    "print(f\"Distribui√ß√£o dos labels: {np.bincount(y_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extra√ß√£o de Caracter√≠sticas Estat√≠sticas\n",
    "\n",
    "Converte sequ√™ncias temporais (90 frames √ó 4 features) em caracter√≠sticas estat√≠sticas para ML cl√°ssico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtratorCaracteristicasFadiga:\n",
    "    \"\"\"Extrai caracter√≠sticas estat√≠sticas de sequ√™ncias temporais para ML cl√°ssico\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nomes_features = NOMES_FEATURES.copy()\n",
    "    \n",
    "    def inclinacao_tendencia(self, sinal):\n",
    "        \"\"\"Calcula inclina√ß√£o da tend√™ncia linear\"\"\"\n",
    "        if len(sinal) < 2:\n",
    "            return 0.0\n",
    "        x = np.arange(len(sinal))\n",
    "        try:\n",
    "            inclinacao, _, _, _, _ = stats.linregress(x, sinal)\n",
    "            return inclinacao if not np.isnan(inclinacao) else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def taxa_cruzamento_zero(self, sinal):\n",
    "        \"\"\"Taxa de cruzamento do zero (medida de variabilidade)\"\"\"\n",
    "        if len(sinal) < 2:\n",
    "            return 0.0\n",
    "        centrado_na_media = sinal - np.mean(sinal)\n",
    "        cruzamentos = np.sum(np.diff(np.sign(centrado_na_media)) != 0)\n",
    "        return cruzamentos / len(sinal)\n",
    "    \n",
    "    def autocorr_lag1(self, sinal):\n",
    "        \"\"\"Autocorrela√ß√£o de lag-1 (consist√™ncia temporal)\"\"\"\n",
    "        if len(sinal) < 3:\n",
    "            return 0.0\n",
    "        try:\n",
    "            corr = np.corrcoef(sinal[:-1], sinal[1:])[0, 1]\n",
    "            return corr if not np.isnan(corr) else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def extrair_features_sinal(self, sinal):\n",
    "        \"\"\"Extrai 11 caracter√≠sticas estat√≠sticas de um √∫nico sinal\"\"\"\n",
    "        features = [\n",
    "            np.mean(sinal),                    # Tend√™ncia central\n",
    "            np.std(sinal),                     # Variabilidade  \n",
    "            np.median(sinal),                  # Tend√™ncia central robusta\n",
    "            np.min(sinal),                     # Medidas de amplitude\n",
    "            np.max(sinal),\n",
    "            np.ptp(sinal),                     # Peak-to-peak (max - min)\n",
    "            np.percentile(sinal, 25),          # Quartis da distribui√ß√£o\n",
    "            np.percentile(sinal, 75),\n",
    "            self.inclinacao_tendencia(sinal), # Din√¢mica temporal\n",
    "            self.taxa_cruzamento_zero(sinal),\n",
    "            self.autocorr_lag1(sinal)          # Correla√ß√£o temporal\n",
    "        ]\n",
    "        return features\n",
    "    \n",
    "    def transform(self, X_sequences):\n",
    "        \"\"\"\n",
    "        Transforma sequ√™ncias temporais em caracter√≠sticas estat√≠sticas\n",
    "        \n",
    "        Input: (n_samples, 90, 4) - sequ√™ncias temporais\n",
    "        Output: (n_samples, 44) - caracter√≠sticas estat√≠sticas\n",
    "        \"\"\"\n",
    "        n_samples = X_sequences.shape[0]\n",
    "        n_features = len(self.nomes_features)\n",
    "        \n",
    "        X_features = np.zeros((n_samples, n_features))\n",
    "        \n",
    "        print(f\"Extraindo caracter√≠sticas para {n_samples} sequ√™ncias...\")\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            if i % 1000 == 0:\n",
    "                print(f\"  Processando amostra {i}/{n_samples}\")\n",
    "            \n",
    "            sequencia = X_sequences[i]  # Shape: (90, 4)\n",
    "            features_amostra = []\n",
    "            \n",
    "            # Extrair features para cada um dos 4 sinais\n",
    "            for idx_sinal in range(4):  # PERCLOS, MAR, BLINK_RATE, HEAD_STABILITY\n",
    "                sinal = sequencia[:, idx_sinal]\n",
    "                features_sinal = self.extrair_features_sinal(sinal)\n",
    "                features_amostra.extend(features_sinal)\n",
    "            \n",
    "            X_features[i] = features_amostra\n",
    "        \n",
    "        print(f\"Extra√ß√£o de caracter√≠sticas conclu√≠da! Shape: {X_features.shape}\")\n",
    "        return X_features\n",
    "\n",
    "# Inicializar extrator\n",
    "extrator_features = ExtratorCaracteristicasFadiga()\n",
    "print(f\"Extrator inicializado com {len(extrator_features.nomes_features)} caracter√≠sticas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extrair Caracter√≠sticas das Sequ√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extrair caracter√≠sticas estat√≠sticas das sequ√™ncias temporais\n",
    "print(\"Extraindo caracter√≠sticas estat√≠sticas das sequ√™ncias temporais...\")\n",
    "print(f\"Formato de entrada: {X_sequences.shape} (amostras, passos_temporais, caracter√≠sticas)\")\n",
    "\n",
    "X_features = extrator_features.transform(X_sequences)\n",
    "\n",
    "print(f\"\\nResultados da extra√ß√£o:\")\n",
    "print(f\"  Sequ√™ncias originais: {X_sequences.shape}\")\n",
    "print(f\"  Caracter√≠sticas extra√≠das: {X_features.shape}\")\n",
    "print(f\"  Features por sinal: 11\")\n",
    "print(f\"  Total de sinais: 4 (PERCLOS, MAR, BLINK_RATE, HEAD_STABILITY)\")\n",
    "\n",
    "# Verificar qualidade dos dados\n",
    "nan_count = np.isnan(X_features).sum()\n",
    "inf_count = np.isinf(X_features).sum()\n",
    "print(f\"\\nVerifica√ß√£o de qualidade:\")\n",
    "print(f\"  Valores NaN: {nan_count}\")\n",
    "print(f\"  Valores infinitos: {inf_count}\")\n",
    "\n",
    "if nan_count > 0 or inf_count > 0:\n",
    "    print(\"  Corrigindo valores NaN/Inf...\")\n",
    "    X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print(\"\\nEstat√≠sticas das caracter√≠sticas:\")\n",
    "print(f\"  M√©dia: {np.mean(X_features):.4f}\")\n",
    "print(f\"  Desvio padr√£o: {np.std(X_features):.4f}\")\n",
    "print(f\"  Min: {np.min(X_features):.4f}\")\n",
    "print(f\"  Max: {np.max(X_features):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Divis√£o e Pr√©-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir dados mantendo balanceamento das classes\n",
    "print(\"Dividindo dados em conjuntos de treino/teste...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_features, y_labels, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_labels\n",
    ")\n",
    "\n",
    "print(f\"Conjunto de treino: {X_train.shape[0]} amostras\")\n",
    "print(f\"Conjunto de teste: {X_test.shape[0]} amostras\")\n",
    "print(f\"Distribui√ß√£o treino: {np.bincount(y_train)}\")\n",
    "print(f\"Distribui√ß√£o teste: {np.bincount(y_test)}\")\n",
    "\n",
    "# Normaliza√ß√£o das caracter√≠sticas\n",
    "print(\"\\nAplicando normaliza√ß√£o...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Dados de treino normalizados: {X_train_scaled.shape}\")\n",
    "print(f\"Dados de teste normalizados: {X_test_scaled.shape}\")\n",
    "\n",
    "# Verificar normaliza√ß√£o\n",
    "print(f\"\\nEstat√≠sticas ap√≥s normaliza√ß√£o (treino):\")\n",
    "print(f\"  M√©dia: {np.mean(X_train_scaled):.6f}\")\n",
    "print(f\"  Desvio padr√£o: {np.std(X_train_scaled):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Treinamento do Modelo Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar Random Forest baseline\n",
    "print(\"Treinando classificador Random Forest baseline...\")\n",
    "\n",
    "rf_baseline = RandomForestClassifier(\n",
    "    n_estimators=100,           # 100 √°rvores\n",
    "    max_depth=10,               # Evitar overfitting\n",
    "    min_samples_split=5,        # Evitar overfitting\n",
    "    min_samples_leaf=2,         # Evitar overfitting\n",
    "    class_weight='balanced',    # Lidar com desbalanceamento\n",
    "    random_state=42,            # Reprodutibilidade\n",
    "    n_jobs=-1                   # Usar todos os cores\n",
    ")\n",
    "\n",
    "# Treinar o modelo\n",
    "rf_baseline.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Treinamento Random Forest conclu√≠do!\")\n",
    "print(f\"N√∫mero de caracter√≠sticas usadas: {rf_baseline.n_features_in_}\")\n",
    "print(f\"N√∫mero de √°rvores: {rf_baseline.n_estimators}\")\n",
    "print(f\"Classes: {rf_baseline.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Avalia√ß√£o do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fazer predi√ß√µes\n",
    "print(\"Avaliando modelo Random Forest baseline...\")\n",
    "\n",
    "y_train_pred = rf_baseline.predict(X_train_scaled)\n",
    "y_test_pred = rf_baseline.predict(X_test_scaled)\n",
    "y_test_proba = rf_baseline.predict_proba(X_test_scaled)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "acuracia_treino = accuracy_score(y_train, y_train_pred)\n",
    "acuracia_teste = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Determinar nomes das classes\n",
    "classes_unicas = np.unique(y_test)\n",
    "print(f\"Classes detectadas: {classes_unicas}\")\n",
    "\n",
    "# Mapear labels para nomes\n",
    "mapeamento_classes = {0: 'Alert', 5: 'Low_Vigilant', 10: 'Drowsy'}\n",
    "nomes_classes = [mapeamento_classes.get(cls, f'Classe_{cls}') for cls in sorted(classes_unicas)]\n",
    "\n",
    "print(f\"\\n=== RESULTADOS BASELINE ===\")\n",
    "print(f\"Acur√°cia Treino: {acuracia_treino:.4f} ({acuracia_treino*100:.2f}%)\")\n",
    "print(f\"Acur√°cia Teste: {acuracia_teste:.4f} ({acuracia_teste*100:.2f}%)\")\n",
    "print(f\"Tipo do problema: {len(classes_unicas)}-classe\")\n",
    "print(f\"Classes: {nomes_classes}\")\n",
    "\n",
    "# Relat√≥rio detalhado\n",
    "print(f\"\\n=== RELAT√ìRIO DETALHADO ===\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=nomes_classes))\n",
    "\n",
    "# Matriz de confus√£o\n",
    "print(f\"\\n=== MATRIZ DE CONFUS√ÉO ===\")\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(cm)\n",
    "print(f\"Linhas: Labels verdadeiros, Colunas: Predi√ß√µes\")\n",
    "print(f\"Ordem das classes: {nomes_classes}\")\n",
    "\n",
    "# ROC AUC\n",
    "try:\n",
    "    if len(classes_unicas) == 2:\n",
    "        roc_auc = roc_auc_score(y_test, y_test_proba[:, 1])\n",
    "        print(f\"\\nROC AUC Score: {roc_auc:.4f}\")\n",
    "    else:\n",
    "        roc_auc = roc_auc_score(y_test, y_test_proba, multi_class='ovr', average='macro')\n",
    "        print(f\"\\nROC AUC Score (macro-avg): {roc_auc:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nROC AUC: N√£o foi poss√≠vel calcular - {str(e)}\")\n",
    "\n",
    "# Avalia√ß√£o de performance\n",
    "print(f\"\\n=== AVALIA√á√ÉO DE PERFORMANCE ===\")\n",
    "if acuracia_teste >= 0.85:\n",
    "    print(\"üü¢ EXCELENTE: Performance de n√≠vel cl√≠nico (‚â•85%)\")\n",
    "elif acuracia_teste >= 0.80:\n",
    "    print(\"üü° MUITO BOM: Performance de alta qualidade (‚â•80%)\")\n",
    "elif acuracia_teste >= 0.75:\n",
    "    print(\"üîµ BOM: Performance aceit√°vel (‚â•75%)\")\n",
    "elif acuracia_teste >= 0.70:\n",
    "    print(\"üü° RAZO√ÅVEL: Performance m√≠nima aceit√°vel (‚â•70%)\")\n",
    "else:\n",
    "    print(\"üî¥ RUIM: Abaixo da performance aceit√°vel (<70%)\")\n",
    "\n",
    "print(f\"Meta: 75-85% de acur√°cia para uso cl√≠nico\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. An√°lise de Import√¢ncia das Caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise de import√¢ncia das caracter√≠sticas\n",
    "print(\"Analisando import√¢ncia das caracter√≠sticas...\")\n",
    "\n",
    "# Obter import√¢ncia das caracter√≠sticas do Random Forest\n",
    "scores_importancia = rf_baseline.feature_importances_\n",
    "df_importancia = pd.DataFrame({\n",
    "    'caracteristica': extrator_features.nomes_features,\n",
    "    'importancia': scores_importancia\n",
    "}).sort_values('importancia', ascending=False)\n",
    "\n",
    "# Mostrar top 20 caracter√≠sticas mais importantes\n",
    "print(f\"\\n=== TOP 20 CARACTER√çSTICAS MAIS IMPORTANTES ===\")\n",
    "print(df_importancia.head(20).to_string(index=False))\n",
    "\n",
    "# Agrupar por tipo de sinal\n",
    "importancia_por_sinal = {}\n",
    "for sinal in NOMES_SINAIS:\n",
    "    features_sinal = df_importancia[df_importancia['caracteristica'].str.startswith(sinal)]\n",
    "    importancia_por_sinal[sinal] = features_sinal['importancia'].sum()\n",
    "\n",
    "print(f\"\\n=== IMPORT√ÇNCIA POR TIPO DE SINAL ===\")\n",
    "for sinal, importancia in sorted(importancia_por_sinal.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{sinal:15} {importancia:.4f} ({importancia*100:.2f}%)\")\n",
    "\n",
    "# Visualizar import√¢ncia das caracter√≠sticas\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Top 15 caracter√≠sticas\n",
    "top_features = df_importancia.head(15)\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.barh(range(len(top_features)), top_features['importancia'])\n",
    "plt.yticks(range(len(top_features)), top_features['caracteristica'])\n",
    "plt.xlabel('Import√¢ncia da Caracter√≠stica')\n",
    "plt.title('Top 15 Caracter√≠sticas Mais Importantes')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "# Import√¢ncia por sinal\n",
    "plt.subplot(2, 1, 2)\n",
    "sinais = list(importancia_por_sinal.keys())\n",
    "importancias = list(importancia_por_sinal.values())\n",
    "plt.bar(sinais, importancias)\n",
    "plt.xlabel('Tipo de Sinal')\n",
    "plt.ylabel('Import√¢ncia Total')\n",
    "plt.title('Import√¢ncia das Caracter√≠sticas por Tipo de Sinal')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Salvar import√¢ncia das caracter√≠sticas\n",
    "df_importancia.to_csv(DIRETORIO_MODELOS / 'importancia_caracteristicas.csv', index=False)\n",
    "print(f\"\\nImport√¢ncia das caracter√≠sticas salva em: {DIRETORIO_MODELOS / 'importancia_caracteristicas.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Valida√ß√£o Cruzada Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valida√ß√£o cruzada temporal para avaliar estabilidade\n",
    "print(\"Executando valida√ß√£o cruzada temporal...\")\n",
    "\n",
    "# Usar TimeSeriesSplit para respeitar ordem temporal\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Scores de valida√ß√£o cruzada\n",
    "cv_scores = cross_val_score(\n",
    "    rf_baseline, X_train_scaled, y_train, \n",
    "    cv=tscv, scoring='accuracy', n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\n=== RESULTADOS DA VALIDA√á√ÉO CRUZADA TEMPORAL ===\")\n",
    "print(f\"Scores individuais: {cv_scores}\")\n",
    "print(f\"Score m√©dio: {cv_scores.mean():.4f} ({cv_scores.mean()*100:.2f}%)\")\n",
    "print(f\"Desvio padr√£o: {cv_scores.std():.4f} (¬±{cv_scores.std()*100:.2f}%)\")\n",
    "print(f\"Intervalo de confian√ßa 95%: [{cv_scores.mean() - 2*cv_scores.std():.4f}, {cv_scores.mean() + 2*cv_scores.std():.4f}]\")\n",
    "\n",
    "# Avalia√ß√£o de estabilidade\n",
    "if cv_scores.std() < 0.05:\n",
    "    print(\"\\nüü¢ EST√ÅVEL: Baixa vari√¢ncia entre folds (<5%)\")\n",
    "elif cv_scores.std() < 0.10:\n",
    "    print(\"\\nüü° MODERADO: Vari√¢ncia aceit√°vel entre folds (<10%)\")\n",
    "else:\n",
    "    print(\"\\nüî¥ INST√ÅVEL: Alta vari√¢ncia entre folds (‚â•10%)\")\n",
    "\n",
    "# Plotar resultados da valida√ß√£o cruzada\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(cv_scores) + 1), cv_scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', label=f'M√©dia: {cv_scores.mean():.3f}')\n",
    "plt.fill_between(range(1, len(cv_scores) + 1), \n",
    "                 cv_scores.mean() - cv_scores.std(), \n",
    "                 cv_scores.mean() + cv_scores.std(), \n",
    "                 alpha=0.2, color='red')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Acur√°cia')\n",
    "plt.title('Scores da Valida√ß√£o Cruzada')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(cv_scores, bins=5, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(cv_scores.mean(), color='red', linestyle='--', linewidth=2, label=f'M√©dia: {cv_scores.mean():.3f}')\n",
    "plt.xlabel('Acur√°cia')\n",
    "plt.ylabel('Frequ√™ncia')\n",
    "plt.title('Distribui√ß√£o dos Scores CV')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Salvar Modelo Treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelos treinados e componentes de pr√©-processamento\n",
    "print(\"Salvando modelos treinados e componentes...\")\n",
    "\n",
    "# Salvar modelo Random Forest baseline\n",
    "caminho_modelo_rf = DIRETORIO_MODELOS / 'rf_baseline_model.joblib'\n",
    "joblib.dump(rf_baseline, caminho_modelo_rf)\n",
    "print(f\"Random Forest baseline salvo em: {caminho_modelo_rf}\")\n",
    "\n",
    "# Salvar o scaler\n",
    "caminho_scaler = DIRETORIO_MODELOS / 'feature_scaler.joblib'\n",
    "joblib.dump(scaler, caminho_scaler)\n",
    "print(f\"Scaler salvo em: {caminho_scaler}\")\n",
    "\n",
    "# Salvar extrator de caracter√≠sticas\n",
    "caminho_extrator = DIRETORIO_MODELOS / 'feature_extractor.joblib'\n",
    "joblib.dump(extrator_features, caminho_extrator)\n",
    "print(f\"Extrator de caracter√≠sticas salvo em: {caminho_extrator}\")\n",
    "\n",
    "# Salvar informa√ß√µes das classes\n",
    "info_classes = {\n",
    "    'classes': rf_baseline.classes_.tolist(),\n",
    "    'n_classes': len(rf_baseline.classes_),\n",
    "    'mapeamento_classes': {0: 'Alert', 5: 'Low_Vigilant', 10: 'Drowsy'}\n",
    "}\n",
    "import json\n",
    "caminho_info_classes = DIRETORIO_MODELOS / 'class_info.json'\n",
    "with open(caminho_info_classes, 'w') as f:\n",
    "    json.dump(info_classes, f, indent=2)\n",
    "print(f\"Informa√ß√µes das classes salvas em: {caminho_info_classes}\")\n",
    "\n",
    "# C√≥digo para pipeline de deploy\n",
    "codigo_deploy = '''import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "class PipelinePredicaoFadiga:\n",
    "    \"\"\"Pipeline completo para predi√ß√£o de fadiga usando Random Forest\"\"\"\n",
    "    \n",
    "    def __init__(self, diretorio_modelo):\n",
    "        diretorio_modelo = Path(diretorio_modelo)\n",
    "        \n",
    "        # Carregar componentes\n",
    "        self.extrator_features = joblib.load(diretorio_modelo / 'feature_extractor.joblib')\n",
    "        self.scaler = joblib.load(diretorio_modelo / 'feature_scaler.joblib')\n",
    "        self.classificador = joblib.load(diretorio_modelo / 'rf_baseline_model.joblib')\n",
    "        \n",
    "        # Carregar informa√ß√µes das classes\n",
    "        with open(diretorio_modelo / 'class_info.json', 'r') as f:\n",
    "            self.info_classes = json.load(f)\n",
    "        \n",
    "        self.classes = self.info_classes['classes']\n",
    "        self.mapeamento_classes = self.info_classes['mapeamento_classes']\n",
    "    \n",
    "    def predizer_sequencia(self, sequencia):\n",
    "        \"\"\"Prediz fadiga de uma √∫nica sequ√™ncia de 90 frames\"\"\"\n",
    "        # Adicionar dimens√£o de batch\n",
    "        sequencias = np.expand_dims(sequencia, axis=0)\n",
    "        \n",
    "        # Extrair caracter√≠sticas\n",
    "        features = self.extrator_features.transform(sequencias)\n",
    "        \n",
    "        # Normalizar caracter√≠sticas\n",
    "        features_normalizadas = self.scaler.transform(features)\n",
    "        \n",
    "        # Fazer predi√ß√£o\n",
    "        predicao = self.classificador.predict(features_normalizadas)[0]\n",
    "        probabilidades_array = self.classificador.predict_proba(features_normalizadas)[0]\n",
    "        \n",
    "        # Criar dicion√°rio de probabilidades\n",
    "        probabilidades = {}\n",
    "        for i, label_classe in enumerate(self.classes):\n",
    "            nome_classe = self.mapeamento_classes.get(str(label_classe), f'Classe_{label_classe}')\n",
    "            probabilidades[nome_classe] = float(probabilidades_array[i])\n",
    "        \n",
    "        # Obter nome da classe predita\n",
    "        nome_classe_predita = self.mapeamento_classes.get(str(predicao), f'Classe_{predicao}')\n",
    "        \n",
    "        return predicao, probabilidades, nome_classe_predita\n",
    "\n",
    "# Exemplo de uso:\n",
    "# pipeline = PipelinePredicaoFadiga('processed_data_uta_rldd_CORRECTED/rf_models')\n",
    "# predicao, probabilidades, nome_classe = pipeline.predizer_sequencia(dados_sequencia)\n",
    "# print(f\"Predi√ß√£o: {nome_classe} (label: {predicao})\")\n",
    "# print(f\"Probabilidades: {probabilidades}\")\n",
    "'''\n",
    "\n",
    "# Salvar c√≥digo de deploy\n",
    "caminho_deploy = DIRETORIO_MODELOS / 'pipeline_deploy.py'\n",
    "with open(caminho_deploy, 'w') as f:\n",
    "    f.write(codigo_deploy)\n",
    "print(f\"Pipeline de deploy salvo em: {caminho_deploy}\")\n",
    "\n",
    "print(f\"\\n=== TREINAMENTO CONCLU√çDO COM SUCESSO ===\")\n",
    "print(f\"‚úÖ Modelo Random Forest treinado e salvo\")\n",
    "print(f\"‚úÖ Pipeline de extra√ß√£o de caracter√≠sticas salvo\")\n",
    "print(f\"‚úÖ Componentes de pr√©-processamento salvos\")\n",
    "print(f\"‚úÖ Informa√ß√µes das classes salvas\")\n",
    "print(f\"‚úÖ C√≥digo de deploy gerado\")\n",
    "print(f\"‚úÖ Performance do modelo: {acuracia_teste:.1%}\")\n",
    "print(f\"\\nLocaliza√ß√£o dos arquivos: {DIRETORIO_MODELOS}\")\n",
    "print(f\"\\nModelo suporta classifica√ß√£o de {len(rf_baseline.classes_)} classes:\")\n",
    "for cls in rf_baseline.classes_:\n",
    "    nome_classe = mapeamento_classes.get(cls, f'Classe_{cls}')\n",
    "    print(f\"  - Classe {cls}: {nome_classe}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
